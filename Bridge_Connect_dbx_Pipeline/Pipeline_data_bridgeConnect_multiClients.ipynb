{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d82011f-6b94-4356-b3c9-1314a3ecceb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_table = \"bridge.bridge_connect.referrals\"\n",
    "\n",
    "clients_config = {\n",
    "    \"agt\": {\n",
    "        \"target_table\": \"deltasharing.agt.references_bridge_connect\",\n",
    "        \"db_values\": [\"db0\",\"db2\"],\n",
    "    },\n",
    "    # \"telio\": {\n",
    "    #     \"target_table\": \"deltasharing_dev.telio.references_bridge_connect\",\n",
    "    #     \"db_values\": [\"db1\"],\n",
    "    # },\n",
    "}\n",
    "\n",
    "all_db_values = []\n",
    "for cfg in clients_config.values():\n",
    "    all_db_values.extend(cfg[\"db_values\"])\n",
    "\n",
    "all_db_values = sorted(set(all_db_values))\n",
    "\n",
    "checkpoint_path = \"/Volumes/bridge/bridge_connect/checkpoint_bc_pipeline\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8b7a8b-883e-4bd8-bb6b-d2479fc82fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "changes_df = (\n",
    "    spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"readChangeData\", \"true\")\n",
    "        .table(source_table)\n",
    "        .where(col(\"_change_type\").isin(\"insert\", \"update_postimage\"))\n",
    "        .where(col(\"source_db\").isin(all_db_values))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d970f00-00e9-4b0e-8058-7af4b7ebac44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc, row_number\n",
    "from pyspark.sql.window import Window\n",
    "def upsert_to_targets(batch_df, batch_id):\n",
    "    if len(batch_df.limit(1).collect()) == 0 : # is empty\n",
    "        return\n",
    "    \n",
    "    for client_name, cfg in clients_config.items():\n",
    "        target_table = cfg[\"target_table\"]\n",
    "        db_vals = cfg[\"db_values\"]\n",
    "\n",
    "        df_client = batch_df.filter(col(\"source_db\").isin(db_vals))\n",
    "\n",
    "        if len(df_client.limit(1).collect()) == 0: # is empty\n",
    "            print(f\"Client {client_name}: aucune ligne pour db in {db_vals} dans ce batch.\")\n",
    "            continue\n",
    "        # rare : mais dans le cas ou on deux mêmes lignes (id +source_db) dans le même batch :\n",
    "        if \"_commit_version\" in df_client.columns: \n",
    "            w = Window.partitionBy(\"id\", \"source_db\").orderBy(desc(\"_commit_version\")) # order by commit version\n",
    "        elif \"_commit_timestamp\" in df_client.columns:\n",
    "            w = Window.partitionBy(\"id\", \"source_db\").orderBy(desc(\"_commit_timestamp\"))# order by commit timestamp\n",
    "        else:\n",
    "            w = Window.partitionBy(\"id\", \"source_db\").orderBy(desc(\"id\")) # order by id\n",
    "\n",
    "        df_client = (\n",
    "            df_client\n",
    "            .withColumn(\"rn\", row_number().over(w))\n",
    "            .filter(col(\"rn\") == 1)\n",
    "            .drop(\"rn\")\n",
    "        )\n",
    "\n",
    "        print(f\"Client {client_name}: traitement de {df_client.count()} lignes \")\n",
    "\n",
    "        view_name = f\"changes_{client_name}\"\n",
    "        df_client.createOrReplaceTempView(view_name)\n",
    "\n",
    "        merge_sql = f\"\"\"\n",
    "            MERGE INTO {target_table} AS t\n",
    "            USING {view_name} AS s\n",
    "              ON  t.id = s.id         \n",
    "              AND t.source_db = s.source_db        \n",
    "            WHEN MATCHED THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        spark.sql(merge_sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09ec7b35-b355-48a3-be13-d4863a9ea82b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    changes_df.writeStream\n",
    "        .foreachBatch(upsert_to_targets)\n",
    "        .option(\"checkpointLocation\", checkpoint_path) \n",
    "        .trigger(availableNow=True)   \n",
    "        .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# -------------------Sommaire d'Execution-----------\n",
    "print(\"\\n===== STREAM SUMMARY =====\")\n",
    "print(\"Status:\", query.status)\n",
    "\n",
    "if query.exception() is not None:\n",
    "    print(\"\\n❌ ERROR OCCURRED:\")\n",
    "    print(query.exception())\n",
    "else:\n",
    "    print(\"\\n✅ STREAM COMPLETED SUCCESSFULLY\")\n",
    "\n",
    "print(\"\\nLast progress event:\")\n",
    "print(query.lastProgress)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7338061252489356,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Pipeline_data_bridgeConnect_multiClients",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
